services:
  llamafactory:
    image: hiyouga/llamafactory:0.9.3
    container_name: llamafactory
    volumes:
      - /data/LLaMA-Factory-v0.9.3/llamafactory/hf_cache:/root/.cache/huggingface
      - /data/LLaMA-Factory-v0.9.3/llamafactory/output:/app/output
      - /data/LLaMA-Factory-v0.9.3/llamafactory/data:/app/data
    ports:
      - "15555:7860"
      - "15556:8000"
    ipc: host
    tty: true
    # shm_size: "16gb"  # ipc: host is set
    stdin_open: true
    command: llamafactory-cli webui
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['2', '3']
            capabilities: [ gpu ]
    restart: unless-stopped
